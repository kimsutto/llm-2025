from fastapi import FastAPI
from fastapi.responses import Response

from pydantic import BaseModel
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# âœ… ëª¨ë¸ & ì¸ë±ìŠ¤ & ë©”íƒ€ë°ì´í„° ë¡œë“œ
# TODO: ëª¨ë¸  ë¡œì»¬, ì›ê²© ë°›ê²Œ ìˆ˜ì • 
# model = SentenceTransformer("intfloat/multilingual-e5-large-instruct")
model = SentenceTransformer("../../multilingual-e5-large-instruct")
index = faiss.read_index("../data/output_faiss/faiss.index")
tokenizer = AutoTokenizer.from_pretrained("../../codet5-base")
generator = AutoModelForSeq2SeqLM.from_pretrained("../../codet5-base")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
generator.to(device)

with open("../data/output_faiss/metadata.pkl", "rb") as f:
    metadata = pickle.load(f)  # âœ… ìš°ë¦¬ êµ¬ì¡°: { file, name, code }

# âœ… FastAPI ê°ì²´ ìƒì„±
app = FastAPI()

# âœ… ìš”ì²­ ìŠ¤í‚¤ë§ˆ
class RAGRequest(BaseModel):
    question: str
    top_k: int = 2

# âœ… LLM ì§ˆë¬¸ ìƒì„± 
def rewrite_query(question: str, embedding_model_name="intfloat/multilingual-e5-large-instruct", vector_db="FAISS") -> str:
    # í”„ë¡¬í”„íŠ¸: ë²¡í„° ëª¨ë¸ê³¼ DB ì •ë³´ë¥¼ ê°™ì´ ì œê³µ
    prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ì í•©í•˜ê²Œ ì§§ê³  ëª…í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë°”ê¿”ì¤˜.

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ê²€ìƒ‰ìš© ì¿¼ë¦¬:"""

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
    outputs = generator.generate(
        inputs["input_ids"],
        max_new_tokens=64,
        num_beams=4,
        early_stopping=True
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… LLM ì‘ë‹µ ìƒì„±
def generate_answer_with_codet5(question: str, context_chunks: list[str]) -> str:
    # âœ… ì‹œìŠ¤í…œ ì—­í•  ê³ ì •
    base_role = f"""ë„ˆëŠ” Vue 2 + TypeScript + class-component ê¸°ë°˜ í”„ë¡œì íŠ¸ì˜ AI ì½”ë“œ ë¹„ì„œì•¼.
ì•„ë˜ ì½”ë“œë¥¼ ì°¸ê³ í•´ì„œ '{question}' ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜."""
    # âœ… ì²­í¬ ìˆ˜ ì œí•œ
    limited_chunks = context_chunks[:2]  # ë„ˆë¬´ ë§ìœ¼ë©´ truncate
    context = "\n\n".join([f"// {i+1}ë²ˆ ì½”ë“œ\n{chunk}" for i, chunk in enumerate(limited_chunks)])

    prompt = f"{base_role}\n\n{context}\n\në‹µë³€:"

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
    outputs = generator.generate(
        inputs["input_ids"],
        max_new_tokens=256,
        num_beams=4,
        early_stopping=True
    )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    clean = ''.join(filter(lambda x: x.isprintable(), result))
    clean = clean.replace('\ufffd', '').strip()

    return clean


# âœ… ê²€ìƒ‰ API
@app.post("/search")
async def search(req: RAGRequest):
    try:
        # 0 ì‚¬ìš©ì ì§ˆë¬¸ ì •ì œ 
        refined_question = rewrite_query(req.question)
        print(refined_question)

        # 1. ì„ë² ë”© ìƒì„± (e5 ê³„ì—´ì€ query: ì ‘ë‘ì–´ í•„ìš”)
        query_prompt = f"query: {refined_question}"
        query_embedding = model.encode([query_prompt], normalize_embeddings=True)
        query_embedding = np.array(query_embedding, dtype="float32")

        # 2. FAISS ê²€ìƒ‰
        distances, indices = index.search(query_embedding, req.top_k)

        # 3. ê²°ê³¼ êµ¬ì„± (LLM ì‘ë‹µ ìƒì„±)
        chunks = []
        references = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx >= len(metadata):
                continue
            entry = metadata[idx]
            chunks.append(entry["code"])
            references.append({
                "file": entry["file"],
                "name": entry["name"],
                "score": float(dist),
                "code": entry["code"]
            })
        answer = generate_answer_with_codet5(req.question, chunks)
        print(answer)


        # 4. Markdown í˜•ì‹ì˜ ì‘ë‹µ ë¬¸ìì—´ êµ¬ì„±
        md_lines = [f"# ì§ˆë¬¸: {req.question}\n", "## ğŸ¤– LLM ì‘ë‹µ", answer, "\n## ğŸ” ì°¸ê³  ì½”ë“œ (Top {})\n".format(len(references))]

        for i, ref in enumerate(references, 1):
            md_lines.append(f"### {i}. ğŸ”¹ {ref['name']} ({ref['file']})")
            md_lines.append(f"**Score:** {ref['score']:.4f}")
            md_lines.append("```ts")
            md_lines.append(ref["code"])
            md_lines.append("```\n")

        md_result = "\n".join(md_lines)

        # 5. ì‘ë‹µ ë°˜í™˜ (Markdown ë¬¸ìì—´ì„ results í•„ë“œë¡œ)
        return Response(content=md_result, media_type="text/markdown")


    except Exception as e:
        # â—ì—ëŸ¬ ì²˜ë¦¬
        return {
            "question": req.question,
            "status": "error",
            "message": str(e)
        }
